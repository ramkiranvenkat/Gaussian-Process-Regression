# -*- coding: utf-8 -*-
"""GPRRamkiran.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HP8rzxDup6HF3msf8ykTFVfElrHBhtzx

The following code illustrates the implementation of Bayesian Optimization algorithm with Gaussian Process Regression (GPR)

Importing the basic libraries to the python code
"""

import numpy as np
import random
import matplotlib.pyplot as plt
import math
import numpy.linalg as la

"""Function (gaussianKernel) implementation of a Gaussian Kernel

Kernel,  $ K(x1,x2) = s*exp(-\frac{(x1-x2)^2}{2 l^2})$

where
$x1, x2$ are data points and $l$ is the variance in the kernel,$s$ some constant
"""

def gaussianKernel(x1,x2,l,s):
  value=-((x1-x2)**2)/(2*l**2) 
  return s**2*math.exp(value)

"""Function (dataKernel) is used to form the co-variance Kernel matrix using the gaussian kernel

Here every element of the Kernel Matrix $K(x1,x2)$ is formed from the data matrix
"""

def dataKernel(x1,x2,l=1,s=1):
  kxx=np.zeros((len(x1),len(x2)))
  for i in range(len(x1)):
    for j in range(len(x2)):
      kxx[i,j]=gaussianKernel(x1[i],x2[j],l,s)
  return kxx

"""Generating Data for Bayesian Opotimization

To illustrate an example, following function (dataGen)

$\frac{x}{2}+sin(\frac{6\pi}{N} x) + 2.7$ is used to generate the data for 
$x = 1,2, .. 49, 50$
"""

def dataGen(n):
  data = []
  for i in range(n):
    data.append([float(i),0.5*i+math.sin(2*math.pi/n * 3 * i)+2.7])
    #outputs [x,f(x)] in np.array
  return np.array(data)

"""Function (dataPlt) is used to plot the data. Comments mentioned in the code"""

def dataPlt(data,iidx,inpdata,oidx,gprdata,gprVar,mp,sp,p):
    plt.figure(figsize=(12,9))
    plt.plot(oidx,data[oidx,1], 'r^', label='Actual Data (Missing)')
    plt.plot(iidx,inpdata, 'g^', label='Input Data Noise')
    plt.plot(oidx,gprdata, 'b*', label='Predicted Data')
    plt.plot(p,mp,'k',label='Mean Curve')
    plt.fill_between(p, mp-sp, mp+sp, alpha=0.2)
    plt.xlabel('X')
    plt.ylabel('Y')
    plt.legend()
    plt.grid()
    plt.show()

"""Code to implement the GPR Algorithm"""

sigma = 0.3
l = 5
s = 2
nodp = 50 # number of data points generated
nos = 30 # Number of points took for GPR input (Input Data size 30 samples taken from 50 generated data)

# Data Generation for 50 samples
data = dataGen(nodp) 

# Randomly selecting (Iterator only) only 30 data samples from the generated data
inputData = random.sample(list(data[:,0]),nos)
inputIndex = [int(i) for i in inputData] # index set of input data
outputIndex = [int(i) for i in list(data[:,0]) if int(i) not in inputIndex] # index set of test data 

# data from the input data (substituting index set into the first column of data)
idata = data[inputIndex,1] 

# Adding measurement noise to data
idata = [ele+random.gauss(0,sigma) for ele in idata]

# Forming the Covariance Kernel matrix with both input data and output data
Kxx  = dataKernel(inputIndex,inputIndex,l=l,s=s)
Kxsx = dataKernel(outputIndex,inputIndex,l=l,s=s)
Kxxs = dataKernel(inputIndex,outputIndex,l=l,s=s)
Kxsxs= dataKernel(outputIndex,outputIndex,l=l,s=s)

#Calculating the mean and variance of the estimate
meanOut = (Kxsx @ la.inv(Kxx + sigma**2 * np.eye(nos))) @ idata
varOut = Kxsxs + (sigma**2 * np.eye(nodp-nos)) - Kxsx @ la.inv(Kxx + sigma**2 * np.eye(nos)) @ Kxxs

# For plotting
XStar = []
for i in range(1000):
  XStar.append(i*50/1000)
XStar = np.array(XStar)

Kxx  = dataKernel(inputIndex,inputIndex,l=l,s=s)
Kxsx = dataKernel(XStar,inputIndex,l=l,s=s)
Kxxs = dataKernel(inputIndex,XStar,l=l,s=s)
Kxsxs= dataKernel(XStar,XStar,l=l,s=s)

#Calculating the mean and variance of the estimate
meanOut1 = (Kxsx @ la.inv(Kxx + sigma**2 * np.eye(nos))) @ data[inputIndex,1]
varOut1 = Kxsxs + (sigma**2 * np.eye(1000)) - Kxsx @ la.inv(Kxx + sigma**2 * np.eye(nos)) @ Kxxs

dataPlt(data,inputIndex,idata,outputIndex,meanOut,varOut,meanOut1,np.sqrt(np.diag(varOut1)),XStar)

"""**Discussion**

1. This algorithm does a good job at the center of data but has higher variance at the edge data points
2. Accuracy increases (Variance decreases) as the number of data points increases
3. There are hyper parameters $s, l$ while defining the gaussian kernel which has to be tuned properly
4. We have implemented a  standalone code but there are several python libraries like Scikit, GPyopt etc in which ready made functions are available for this Bayesian Optimization

***References***
1. https://www.youtube.com/watch?v=uPsAU57tTlw
2. https://machinelearningmastery.com/what-is-bayesian-optimization/
3. http://krasserm.github.io/2018/03/21/bayesian-optimization/
"""